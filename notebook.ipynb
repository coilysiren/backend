{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fea3ecb6-b18c-459d-84b2-1beb432077b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-04-02 23:39:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcache                         \u001b[0m \u001b[36madjective\u001b[0m=\u001b[35mhit\u001b[0m \u001b[36mkey\u001b[0m=\u001b[35mbsky.get-author-feed-text--coilysiren.me\u001b[0m \u001b[36mprefix\u001b[0m=\u001b[35mbsky.get-author-feed-text-\u001b[0m \u001b[36msuffix\u001b[0m=\u001b[35mcoilysiren.me\u001b[0m\n",
      "cursor: 2025-03-28T14:43:31.185Z\n",
      "\u001b[2m2025-04-02 23:39:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcache                         \u001b[0m \u001b[36madjective\u001b[0m=\u001b[35mhit\u001b[0m \u001b[36mkey\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-28T14:43:31.185Z-coilysiren.me\u001b[0m \u001b[36mprefix\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-28T14:43:31.185Z\u001b[0m \u001b[36msuffix\u001b[0m=\u001b[35mcoilysiren.me\u001b[0m\n",
      "cursor: 2025-03-25T11:18:08.551Z\n",
      "\u001b[2m2025-04-02 23:39:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcache                         \u001b[0m \u001b[36madjective\u001b[0m=\u001b[35mhit\u001b[0m \u001b[36mkey\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-25T11:18:08.551Z-coilysiren.me\u001b[0m \u001b[36mprefix\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-25T11:18:08.551Z\u001b[0m \u001b[36msuffix\u001b[0m=\u001b[35mcoilysiren.me\u001b[0m\n",
      "cursor: 2025-03-22T15:51:30.34Z\n",
      "\u001b[2m2025-04-02 23:39:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcache                         \u001b[0m \u001b[36madjective\u001b[0m=\u001b[35mhit\u001b[0m \u001b[36mkey\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-22T15:51:30.34Z-coilysiren.me\u001b[0m \u001b[36mprefix\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-22T15:51:30.34Z\u001b[0m \u001b[36msuffix\u001b[0m=\u001b[35mcoilysiren.me\u001b[0m\n",
      "cursor: 2025-03-14T12:32:15.902Z\n",
      "\u001b[2m2025-04-02 23:39:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcache                         \u001b[0m \u001b[36madjective\u001b[0m=\u001b[35mhit\u001b[0m \u001b[36mkey\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-14T12:32:15.902Z-coilysiren.me\u001b[0m \u001b[36mprefix\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-14T12:32:15.902Z\u001b[0m \u001b[36msuffix\u001b[0m=\u001b[35mcoilysiren.me\u001b[0m\n",
      "cursor: 2025-03-01T17:30:19.824Z\n",
      "\u001b[2m2025-04-02 23:39:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcache                         \u001b[0m \u001b[36madjective\u001b[0m=\u001b[35mhit\u001b[0m \u001b[36mkey\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-01T17:30:19.824Z-coilysiren.me\u001b[0m \u001b[36mprefix\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-01T17:30:19.824Z\u001b[0m \u001b[36msuffix\u001b[0m=\u001b[35mcoilysiren.me\u001b[0m\n",
      "cursor: 2025-01-23T19:45:14.016Z\n",
      "\u001b[2m2025-04-02 23:39:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcache                         \u001b[0m \u001b[36madjective\u001b[0m=\u001b[35mhit\u001b[0m \u001b[36mkey\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-01-23T19:45:14.016Z-coilysiren.me\u001b[0m \u001b[36mprefix\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-01-23T19:45:14.016Z\u001b[0m \u001b[36msuffix\u001b[0m=\u001b[35mcoilysiren.me\u001b[0m\n",
      "cursor: None\n"
     ]
    }
   ],
   "source": [
    "import src.bsky\n",
    "\n",
    "bsky_client = src.bsky.init()\n",
    "\n",
    "handle = \"coilysiren.me\"\n",
    "text = \"\\n\".join(await src.bsky.get_author_feed_texts(bsky_client, \"coilysiren.me\", 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a2e779a-e10d-4eb8-8782-69d025ae739e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/kai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0c839ec-3fce-411c-9424-93b69d859843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import spacy\n",
    "\n",
    "subprocess.run([\"spacy\", \"download\", \"en_core_web_lg\"])\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79d5f640-9a2d-4ba3-b7c2-af604970ec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.06226594632005874, 'server'),\n",
      " (0.061990562601449806, 'dems'),\n",
      " (0.06125068920237876, 'machine learning'),\n",
      " (0.06092129394242671, 'stop'),\n",
      " (0.06064442032810387, 'transgender mice'),\n",
      " (0.05998185380269443, 'social'),\n",
      " (0.0595197027028411, 'trans girl website'),\n",
      " (0.05934486133227008, 'devops engineer'),\n",
      " (0.058274659127150255, 'mobile app'),\n",
      " (0.058180496490271606, 'nextjs'),\n",
      " (0.05796732805901702, 'code'),\n",
      " (0.05666084242192651, 'github.com'),\n",
      " (0.05556134271131322, 'coilysiren'),\n",
      " (0.05518109135534268, 'leetcode'),\n",
      " (0.05241001038254615, 'redis'),\n",
      " (0.05173381567154017, 'twitter'),\n",
      " (0.05048643532961024, 'work laptop'),\n",
      " (0.050215513373595644, 'country'),\n",
      " (0.048908601682541, 'hate'),\n",
      " (0.046409077420662186, 'job'),\n",
      " (0.04540637654603328, 'states'),\n",
      " (0.044819379586473126, 'suggestions'),\n",
      " (0.042693420864084085, 'public'),\n",
      " (0.041594098622339815, 'working'),\n",
      " (0.04133681437705453, 'problem'),\n",
      " (0.03576566560065, 'bluesky apps'),\n",
      " (0.0354691635625986, 'works'),\n",
      " (0.03502830231476305, 'data'),\n",
      " (0.03178446358222329, 'state'),\n",
      " (0.031138763972341763, 'signal'),\n",
      " (0.028495705287627102, 'world'),\n",
      " (0.027989179834096174, 'black'),\n",
      " (0.02475118873968503, 'react'),\n",
      " (0.02466536845748374, 'love'),\n",
      " (0.021653653493937174, 'game')]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import spacy\n",
    "import yaml\n",
    "import pprint\n",
    "\n",
    "def remove_substring_entries(keyword_list):\n",
    "    # Sort by length of keyword (longest first), then by score (higher is better)\n",
    "    keyword_list.sort(key=lambda x: (-x[0], -len(x[1])))\n",
    "\n",
    "    filtered_keywords = []\n",
    "    seen_words = set()\n",
    "\n",
    "    for score, keyword in keyword_list:\n",
    "        words = set(keyword.split())  # Split phrase into individual words\n",
    "        if not any(word in seen_words for word in words):\n",
    "            filtered_keywords.append((score, keyword))\n",
    "            seen_words.update(words)  # Add words to seen set\n",
    "\n",
    "    return filtered_keywords\n",
    "\n",
    "with open(\"stopwords.yml\", \"r\") as _file:\n",
    "    custom_stopwords = yaml.load(_file, yaml.Loader)\n",
    "\n",
    "yake_kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"en\",\n",
    "    top=50,\n",
    "    dedupLim=1,\n",
    "    stopwords=set(nltk.corpus.stopwords.words(\"english\") + list(nlp.Defaults.stop_words) + custom_stopwords)\n",
    ")\n",
    "keywords = yake_kw_extractor.extract_keywords(text.lower())\n",
    "\n",
    "keywords = [\n",
    "    (pair[0], pair[1]) if type(pair[0]) != str else (pair[1], pair[0])\n",
    "    for pair in keywords\n",
    "] # this pair comes on in a different order depending on your operating system\n",
    "\n",
    "keywords = remove_substring_entries(keywords)\n",
    "\n",
    "print(pprint.pprint(keywords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "506ce211-2715-43d1-b4ea-3a41364f2105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/37/h0w8431d5dqb2pjq5ws463q00000gn/T/ipykernel_1384/2524784109.py:31: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  emoji_score = _keyword.similarity(_emoji)  # Otherwise, use semantic similarity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[work laptop, 0.8737037669212877, 'ğŸ’»', 'laptop'],\n",
      " [game, 0.8588475456114495, 'ğŸ®', 'video game'],\n",
      " [black, 0.8536479492993099, 'âš«', 'black circle'],\n",
      " [stop, 0.8452153448480786, 'ğŸ›‘', 'stop sign'],\n",
      " [mobile app, 0.8229440987707605, 'ğŸ“±', 'mobile phone'],\n",
      " [love, 0.7793053496256116, 'ğŸ’Œ', 'love letter'],\n",
      " [world, 0.7714790030477227, 'ğŸ—ºï¸', 'world map'],\n",
      " [states, 0.739881039640825, 'ğŸ‡ºğŸ‡¸', 'flag: United States'],\n",
      " [trans girl website, 0.7209500023475277, 'ğŸ‘§', 'girl'],\n",
      " [machine learning, 0.6786142029555245, 'ğŸ°', 'slot machine'],\n",
      " [data, 0.6405320669100674, 'ğŸ”¢', 'input numbers'],\n",
      " [job, 0.6342265489954826, 'ğŸ‘¨\\u200dğŸ’¼', 'man office worker'],\n",
      " [signal, 0.633443405414455, 'ğŸ”£', 'input symbols'],\n",
      " [state, 0.6324717996393164, 'ğŸ‡ºğŸ‡¸', 'flag: United States'],\n",
      " [country, 0.6253653921115787, 'ğŸ‡¹ğŸ‡«', 'flag: French Southern Territories'],\n",
      " [working, 0.6105500971586605, 'ğŸƒ\\u200dâ¡ï¸', 'person running facing right'],\n",
      " [hate, 0.6098208879777762, 'ğŸ˜±', 'face screaming in fear'],\n",
      " [public, 0.596301335524919, 'ğŸï¸', 'national park'],\n",
      " [problem, 0.5881406498700572, 'ğŸƒ\\u200dâ¡ï¸', 'person running facing right'],\n",
      " [server, 0.578438574944086, 'ğŸ’½', 'computer disk'],\n",
      " [transgender mice, 0.5703636557225328, 'âš§ï¸', 'transgender symbol'],\n",
      " [works, 0.5452258463620134, 'âœï¸', 'writing hand'],\n",
      " [social, 0.5328209291896988, 'ğŸ§‘\\u200dâš•ï¸', 'health worker'],\n",
      " [react, 0.5265886955490983, 'ğŸ˜Ÿ', 'worried face'],\n",
      " [devops engineer, 0.502307882711038, 'ğŸ§‘\\u200dğŸ’»', 'technologist'],\n",
      " [code, 0.4989283419237469, 'ğŸ—ƒï¸', 'card file box'],\n",
      " [suggestions, 0.48196037944165904, 'â„¹ï¸', 'information'],\n",
      " [twitter, 0.4379332980316985, 'ğŸ”—', 'link'],\n",
      " [bluesky apps, 0.3798560967568783, 'â˜ï¸', 'cloud'],\n",
      " [redis, 0.3051989560227008, 'ğŸ¡', 'blowfish'],\n",
      " [dems, 0.29261593660412655, '\\U0001faf8', 'rightwards pushing hand'],\n",
      " [github.com, 0.23714598288759234, 'ğŸ—¿', 'moai'],\n",
      " [nextjs, 0.0, 'ğŸ˜€', 'grinning face'],\n",
      " [coilysiren, 0.0, 'ğŸ˜€', 'grinning face'],\n",
      " [leetcode, 0.0, 'ğŸ˜€', 'grinning face']]\n",
      "coilysiren.me talks about...\n",
      "ğŸ’» work laptop\n",
      "ğŸ® game\n",
      "âš« black\n",
      "ğŸ›‘ stop\n",
      "ğŸ“± mobile app\n",
      "ğŸ’Œ love\n",
      "ğŸ—ºï¸ world\n",
      "ğŸ‡ºğŸ‡¸ states\n",
      "ğŸ‘§ trans girl website\n",
      "ğŸ° machine learning\n",
      "ğŸ”¢ data\n",
      "ğŸ‘¨â€ğŸ’¼ job\n",
      "ğŸ”£ signal\n",
      "ğŸ‡ºğŸ‡¸ state\n",
      "ğŸ‡¹ğŸ‡« country\n",
      "ğŸƒâ€â¡ï¸ working\n",
      "ğŸ˜± hate\n",
      "ğŸï¸ public\n",
      "ğŸƒâ€â¡ï¸ problem\n",
      "ğŸ’½ server\n",
      "âš§ï¸ transgender mice\n",
      "âœï¸ works\n",
      "ğŸ§‘â€âš•ï¸ social\n",
      "ğŸ˜Ÿ react\n",
      "ğŸ§‘â€ğŸ’» devops engineer\n",
      "ğŸ—ƒï¸ code\n",
      "â„¹ï¸ suggestions\n",
      "ğŸ”— twitter\n",
      "â˜ï¸ bluesky apps\n",
      "ğŸ¡ redis\n",
      "ğŸ«¸ dems\n",
      "ğŸ—¿ github.com\n",
      "ğŸ˜€ nextjs\n",
      "ğŸ˜€ coilysiren\n",
      "ğŸ˜€ leetcode\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")  # Load large model for better similarity\n",
    "\n",
    "# Load emojis from JSON\n",
    "with open(\"emojis.json\", \"r\") as _file:\n",
    "    emojis = json.loads(_file.read())\n",
    "\n",
    "emoji_scores = []\n",
    "\n",
    "# Precompute NLP embeddings for emoji descriptions\n",
    "emoji_nlp = [\n",
    "    nlp(emojis[emoji_index][\"description\"])\n",
    "    for emoji_index in range(len(emojis))\n",
    "]\n",
    "\n",
    "for keyword in keywords:\n",
    "    _keyword = nlp(keyword[1])  # Process keyword text\n",
    "    keyword_scores = []\n",
    "\n",
    "    for emoji_index in range(len(emojis)):\n",
    "        _emoji = emoji_nlp[emoji_index]  # Process emoji description\n",
    "        keyword_score = keyword[0]\n",
    "\n",
    "        # **Exact match logic**: If the keyword matches the emoji description exactly\n",
    "        if keyword[1].lower() == emojis[emoji_index][\"description\"].lower():\n",
    "            emoji_score = 1.0  # Exact match gets max similarity\n",
    "        else:\n",
    "            emoji_score = _keyword.similarity(_emoji)  # Otherwise, use semantic similarity\n",
    "\n",
    "        keyword_scores.append([\n",
    "            _keyword, emoji_score, emojis[emoji_index][\"emoji\"], emojis[emoji_index][\"description\"]\n",
    "        ])\n",
    "\n",
    "    # Sort emoji matches by highest similarity\n",
    "    keyword_scores.sort(key=lambda x: -x[1])\n",
    "    \n",
    "    # Append best match to emoji_scores\n",
    "    emoji_scores.append(keyword_scores[0])\n",
    "\n",
    "# Sort overall results by highest similarity\n",
    "emoji_scores.sort(key=lambda x: -x[1])\n",
    "\n",
    "# Print results\n",
    "pprint.pprint(emoji_scores)\n",
    "\n",
    "print(f\"{handle} talks about...\")\n",
    "\n",
    "for emoji_score in emoji_scores:\n",
    "    print(emoji_score[2], emoji_score[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dcf67d-ce2e-4c5e-89c2-b934af2eb155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
