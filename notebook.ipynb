{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fea3ecb6-b18c-459d-84b2-1beb432077b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-04-02 23:39:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcache                         \u001b[0m \u001b[36madjective\u001b[0m=\u001b[35mhit\u001b[0m \u001b[36mkey\u001b[0m=\u001b[35mbsky.get-author-feed-text--coilysiren.me\u001b[0m \u001b[36mprefix\u001b[0m=\u001b[35mbsky.get-author-feed-text-\u001b[0m \u001b[36msuffix\u001b[0m=\u001b[35mcoilysiren.me\u001b[0m\n",
      "cursor: 2025-03-28T14:43:31.185Z\n",
      "\u001b[2m2025-04-02 23:39:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcache                         \u001b[0m \u001b[36madjective\u001b[0m=\u001b[35mhit\u001b[0m \u001b[36mkey\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-28T14:43:31.185Z-coilysiren.me\u001b[0m \u001b[36mprefix\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-28T14:43:31.185Z\u001b[0m \u001b[36msuffix\u001b[0m=\u001b[35mcoilysiren.me\u001b[0m\n",
      "cursor: 2025-03-25T11:18:08.551Z\n",
      "\u001b[2m2025-04-02 23:39:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcache                         \u001b[0m \u001b[36madjective\u001b[0m=\u001b[35mhit\u001b[0m \u001b[36mkey\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-25T11:18:08.551Z-coilysiren.me\u001b[0m \u001b[36mprefix\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-25T11:18:08.551Z\u001b[0m \u001b[36msuffix\u001b[0m=\u001b[35mcoilysiren.me\u001b[0m\n",
      "cursor: 2025-03-22T15:51:30.34Z\n",
      "\u001b[2m2025-04-02 23:39:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcache                         \u001b[0m \u001b[36madjective\u001b[0m=\u001b[35mhit\u001b[0m \u001b[36mkey\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-22T15:51:30.34Z-coilysiren.me\u001b[0m \u001b[36mprefix\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-22T15:51:30.34Z\u001b[0m \u001b[36msuffix\u001b[0m=\u001b[35mcoilysiren.me\u001b[0m\n",
      "cursor: 2025-03-14T12:32:15.902Z\n",
      "\u001b[2m2025-04-02 23:39:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcache                         \u001b[0m \u001b[36madjective\u001b[0m=\u001b[35mhit\u001b[0m \u001b[36mkey\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-14T12:32:15.902Z-coilysiren.me\u001b[0m \u001b[36mprefix\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-14T12:32:15.902Z\u001b[0m \u001b[36msuffix\u001b[0m=\u001b[35mcoilysiren.me\u001b[0m\n",
      "cursor: 2025-03-01T17:30:19.824Z\n",
      "\u001b[2m2025-04-02 23:39:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcache                         \u001b[0m \u001b[36madjective\u001b[0m=\u001b[35mhit\u001b[0m \u001b[36mkey\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-01T17:30:19.824Z-coilysiren.me\u001b[0m \u001b[36mprefix\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-03-01T17:30:19.824Z\u001b[0m \u001b[36msuffix\u001b[0m=\u001b[35mcoilysiren.me\u001b[0m\n",
      "cursor: 2025-01-23T19:45:14.016Z\n",
      "\u001b[2m2025-04-02 23:39:08\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcache                         \u001b[0m \u001b[36madjective\u001b[0m=\u001b[35mhit\u001b[0m \u001b[36mkey\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-01-23T19:45:14.016Z-coilysiren.me\u001b[0m \u001b[36mprefix\u001b[0m=\u001b[35mbsky.get-author-feed-text-2025-01-23T19:45:14.016Z\u001b[0m \u001b[36msuffix\u001b[0m=\u001b[35mcoilysiren.me\u001b[0m\n",
      "cursor: None\n"
     ]
    }
   ],
   "source": [
    "import src.bsky\n",
    "\n",
    "bsky_client = src.bsky.init()\n",
    "\n",
    "handle = \"coilysiren.me\"\n",
    "text = \"\\n\".join(await src.bsky.get_author_feed_texts(bsky_client, \"coilysiren.me\", 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a2e779a-e10d-4eb8-8782-69d025ae739e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/kai/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0c839ec-3fce-411c-9424-93b69d859843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import spacy\n",
    "\n",
    "subprocess.run([\"spacy\", \"download\", \"en_core_web_lg\"])\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79d5f640-9a2d-4ba3-b7c2-af604970ec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.06226594632005874, 'server'),\n",
      " (0.061990562601449806, 'dems'),\n",
      " (0.06125068920237876, 'machine learning'),\n",
      " (0.06092129394242671, 'stop'),\n",
      " (0.06064442032810387, 'transgender mice'),\n",
      " (0.05998185380269443, 'social'),\n",
      " (0.0595197027028411, 'trans girl website'),\n",
      " (0.05934486133227008, 'devops engineer'),\n",
      " (0.058274659127150255, 'mobile app'),\n",
      " (0.058180496490271606, 'nextjs'),\n",
      " (0.05796732805901702, 'code'),\n",
      " (0.05666084242192651, 'github.com'),\n",
      " (0.05556134271131322, 'coilysiren'),\n",
      " (0.05518109135534268, 'leetcode'),\n",
      " (0.05241001038254615, 'redis'),\n",
      " (0.05173381567154017, 'twitter'),\n",
      " (0.05048643532961024, 'work laptop'),\n",
      " (0.050215513373595644, 'country'),\n",
      " (0.048908601682541, 'hate'),\n",
      " (0.046409077420662186, 'job'),\n",
      " (0.04540637654603328, 'states'),\n",
      " (0.044819379586473126, 'suggestions'),\n",
      " (0.042693420864084085, 'public'),\n",
      " (0.041594098622339815, 'working'),\n",
      " (0.04133681437705453, 'problem'),\n",
      " (0.03576566560065, 'bluesky apps'),\n",
      " (0.0354691635625986, 'works'),\n",
      " (0.03502830231476305, 'data'),\n",
      " (0.03178446358222329, 'state'),\n",
      " (0.031138763972341763, 'signal'),\n",
      " (0.028495705287627102, 'world'),\n",
      " (0.027989179834096174, 'black'),\n",
      " (0.02475118873968503, 'react'),\n",
      " (0.02466536845748374, 'love'),\n",
      " (0.021653653493937174, 'game')]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "import spacy\n",
    "import yaml\n",
    "import pprint\n",
    "\n",
    "def remove_substring_entries(keyword_list):\n",
    "    # Sort by length of keyword (longest first), then by score (higher is better)\n",
    "    keyword_list.sort(key=lambda x: (-x[0], -len(x[1])))\n",
    "\n",
    "    filtered_keywords = []\n",
    "    seen_words = set()\n",
    "\n",
    "    for score, keyword in keyword_list:\n",
    "        words = set(keyword.split())  # Split phrase into individual words\n",
    "        if not any(word in seen_words for word in words):\n",
    "            filtered_keywords.append((score, keyword))\n",
    "            seen_words.update(words)  # Add words to seen set\n",
    "\n",
    "    return filtered_keywords\n",
    "\n",
    "with open(\"stopwords.yml\", \"r\") as _file:\n",
    "    custom_stopwords = yaml.load(_file, yaml.Loader)\n",
    "\n",
    "yake_kw_extractor = yake.KeywordExtractor(\n",
    "    lan=\"en\",\n",
    "    top=50,\n",
    "    dedupLim=1,\n",
    "    stopwords=set(nltk.corpus.stopwords.words(\"english\") + list(nlp.Defaults.stop_words) + custom_stopwords)\n",
    ")\n",
    "keywords = yake_kw_extractor.extract_keywords(text.lower())\n",
    "\n",
    "keywords = [\n",
    "    (pair[0], pair[1]) if type(pair[0]) != str else (pair[1], pair[0])\n",
    "    for pair in keywords\n",
    "] # this pair comes on in a different order depending on your operating system\n",
    "\n",
    "keywords = remove_substring_entries(keywords)\n",
    "\n",
    "print(pprint.pprint(keywords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "506ce211-2715-43d1-b4ea-3a41364f2105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/37/h0w8431d5dqb2pjq5ws463q00000gn/T/ipykernel_1384/2524784109.py:31: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  emoji_score = _keyword.similarity(_emoji)  # Otherwise, use semantic similarity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[work laptop, 0.8737037669212877, 'üíª', 'laptop'],\n",
      " [game, 0.8588475456114495, 'üéÆ', 'video game'],\n",
      " [black, 0.8536479492993099, '‚ö´', 'black circle'],\n",
      " [stop, 0.8452153448480786, 'üõë', 'stop sign'],\n",
      " [mobile app, 0.8229440987707605, 'üì±', 'mobile phone'],\n",
      " [love, 0.7793053496256116, 'üíå', 'love letter'],\n",
      " [world, 0.7714790030477227, 'üó∫Ô∏è', 'world map'],\n",
      " [states, 0.739881039640825, 'üá∫üá∏', 'flag: United States'],\n",
      " [trans girl website, 0.7209500023475277, 'üëß', 'girl'],\n",
      " [machine learning, 0.6786142029555245, 'üé∞', 'slot machine'],\n",
      " [data, 0.6405320669100674, 'üî¢', 'input numbers'],\n",
      " [job, 0.6342265489954826, 'üë®\\u200düíº', 'man office worker'],\n",
      " [signal, 0.633443405414455, 'üî£', 'input symbols'],\n",
      " [state, 0.6324717996393164, 'üá∫üá∏', 'flag: United States'],\n",
      " [country, 0.6253653921115787, 'üáπüá´', 'flag: French Southern Territories'],\n",
      " [working, 0.6105500971586605, 'üèÉ\\u200d‚û°Ô∏è', 'person running facing right'],\n",
      " [hate, 0.6098208879777762, 'üò±', 'face screaming in fear'],\n",
      " [public, 0.596301335524919, 'üèûÔ∏è', 'national park'],\n",
      " [problem, 0.5881406498700572, 'üèÉ\\u200d‚û°Ô∏è', 'person running facing right'],\n",
      " [server, 0.578438574944086, 'üíΩ', 'computer disk'],\n",
      " [transgender mice, 0.5703636557225328, '‚ößÔ∏è', 'transgender symbol'],\n",
      " [works, 0.5452258463620134, '‚úçÔ∏è', 'writing hand'],\n",
      " [social, 0.5328209291896988, 'üßë\\u200d‚öïÔ∏è', 'health worker'],\n",
      " [react, 0.5265886955490983, 'üòü', 'worried face'],\n",
      " [devops engineer, 0.502307882711038, 'üßë\\u200düíª', 'technologist'],\n",
      " [code, 0.4989283419237469, 'üóÉÔ∏è', 'card file box'],\n",
      " [suggestions, 0.48196037944165904, '‚ÑπÔ∏è', 'information'],\n",
      " [twitter, 0.4379332980316985, 'üîó', 'link'],\n",
      " [bluesky apps, 0.3798560967568783, '‚òÅÔ∏è', 'cloud'],\n",
      " [redis, 0.3051989560227008, 'üê°', 'blowfish'],\n",
      " [dems, 0.29261593660412655, '\\U0001faf8', 'rightwards pushing hand'],\n",
      " [github.com, 0.23714598288759234, 'üóø', 'moai'],\n",
      " [nextjs, 0.0, 'üòÄ', 'grinning face'],\n",
      " [coilysiren, 0.0, 'üòÄ', 'grinning face'],\n",
      " [leetcode, 0.0, 'üòÄ', 'grinning face']]\n",
      "coilysiren.me talks about...\n",
      "üíª work laptop\n",
      "üéÆ game\n",
      "‚ö´ black\n",
      "üõë stop\n",
      "üì± mobile app\n",
      "üíå love\n",
      "üó∫Ô∏è world\n",
      "üá∫üá∏ states\n",
      "üëß trans girl website\n",
      "üé∞ machine learning\n",
      "üî¢ data\n",
      "üë®‚Äçüíº job\n",
      "üî£ signal\n",
      "üá∫üá∏ state\n",
      "üáπüá´ country\n",
      "üèÉ‚Äç‚û°Ô∏è working\n",
      "üò± hate\n",
      "üèûÔ∏è public\n",
      "üèÉ‚Äç‚û°Ô∏è problem\n",
      "üíΩ server\n",
      "‚ößÔ∏è transgender mice\n",
      "‚úçÔ∏è works\n",
      "üßë‚Äç‚öïÔ∏è social\n",
      "üòü react\n",
      "üßë‚Äçüíª devops engineer\n",
      "üóÉÔ∏è code\n",
      "‚ÑπÔ∏è suggestions\n",
      "üîó twitter\n",
      "‚òÅÔ∏è bluesky apps\n",
      "üê° redis\n",
      "ü´∏ dems\n",
      "üóø github.com\n",
      "üòÄ nextjs\n",
      "üòÄ coilysiren\n",
      "üòÄ leetcode\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")  # Load large model for better similarity\n",
    "\n",
    "# Load emojis from JSON\n",
    "with open(\"emojis.json\", \"r\") as _file:\n",
    "    emojis = json.loads(_file.read())\n",
    "\n",
    "emoji_scores = []\n",
    "\n",
    "# Precompute NLP embeddings for emoji descriptions\n",
    "emoji_nlp = [\n",
    "    nlp(emojis[emoji_index][\"description\"])\n",
    "    for emoji_index in range(len(emojis))\n",
    "]\n",
    "\n",
    "for keyword in keywords:\n",
    "    _keyword = nlp(keyword[1])  # Process keyword text\n",
    "    keyword_scores = []\n",
    "\n",
    "    for emoji_index in range(len(emojis)):\n",
    "        _emoji = emoji_nlp[emoji_index]  # Process emoji description\n",
    "        keyword_score = keyword[0]\n",
    "\n",
    "        # **Exact match logic**: If the keyword matches the emoji description exactly\n",
    "        if keyword[1].lower() == emojis[emoji_index][\"description\"].lower():\n",
    "            emoji_score = 1.0  # Exact match gets max similarity\n",
    "        else:\n",
    "            emoji_score = _keyword.similarity(_emoji)  # Otherwise, use semantic similarity\n",
    "\n",
    "        keyword_scores.append([\n",
    "            _keyword, emoji_score, emojis[emoji_index][\"emoji\"], emojis[emoji_index][\"description\"]\n",
    "        ])\n",
    "\n",
    "    # Sort emoji matches by highest similarity\n",
    "    keyword_scores.sort(key=lambda x: -x[1])\n",
    "    \n",
    "    # Append best match to emoji_scores\n",
    "    emoji_scores.append(keyword_scores[0])\n",
    "\n",
    "# Sort overall results by highest similarity\n",
    "emoji_scores.sort(key=lambda x: -x[1])\n",
    "\n",
    "# Print results\n",
    "pprint.pprint(emoji_scores)\n",
    "\n",
    "print(f\"{handle} talks about...\")\n",
    "\n",
    "for emoji_score in emoji_scores:\n",
    "    print(emoji_score[2], emoji_score[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dcf67d-ce2e-4c5e-89c2-b934af2eb155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
